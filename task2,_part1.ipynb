{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task2, part1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2hWEpLXLiW5",
        "colab_type": "code",
        "outputId": "8398a11f-a0ee-4307-edf8-ab07e2fc10ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -U pip\n",
        "!pip install -U dill\n",
        "!pip install -U nltk==3.5b1\n",
        "!pip install lyricsgenius\n",
        "!pip install git+https://github.com/johnwmillr/LyricsGenius.git\n",
        "#installed all the necessary libraries"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/0c/d01aa759fdc501a58f431eb594a17495f15b88da142ce14b5845662c13f3/pip-20.0.2-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-20.0.2\n",
            "Requirement already up-to-date: dill in /usr/local/lib/python3.6/dist-packages (0.3.1.1)\n",
            "Collecting nltk==3.5b1\n",
            "  Downloading nltk-3.5b1.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from nltk==3.5b1) (7.1.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from nltk==3.5b1) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from nltk==3.5b1) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk==3.5b1) (4.38.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5b1-py3-none-any.whl size=1434695 sha256=6f48036e369686eb7479c45ba3cd0d693ef99793552aca7340040ad209339cdf\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/86/25/c46dd9a7eced213ff414cc8266b44bb26e61f3f787d36d8dfc\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.5b1\n",
            "Collecting lyricsgenius\n",
            "  Downloading lyricsgenius-1.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from lyricsgenius) (2.21.0)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 409 kB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius) (2.8)\n",
            "Installing collected packages: beautifulsoup4, lyricsgenius\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed beautifulsoup4-4.6.0 lyricsgenius-1.8.2\n",
            "Collecting git+https://github.com/johnwmillr/LyricsGenius.git\n",
            "  Cloning https://github.com/johnwmillr/LyricsGenius.git to /tmp/pip-req-build-awgt3g2o\n",
            "  Running command git clone -q https://github.com/johnwmillr/LyricsGenius.git /tmp/pip-req-build-awgt3g2o\n",
            "Requirement already satisfied (use --upgrade to upgrade): lyricsgenius==1.8.2 from git+https://github.com/johnwmillr/LyricsGenius.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from lyricsgenius==1.8.2) (4.6.0)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from lyricsgenius==1.8.2) (2.21.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius==1.8.2) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius==1.8.2) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius==1.8.2) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius==1.8.2) (3.0.4)\n",
            "Building wheels for collected packages: lyricsgenius\n",
            "  Building wheel for lyricsgenius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lyricsgenius: filename=lyricsgenius-1.8.2-py3-none-any.whl size=15038 sha256=aaadbd4e238d9f1e0c9ee099426ec0f7cabecc9e1302bc27763565977d28c2b7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rt3qgiwj/wheels/58/60/c5/816302a2fe1ddc34a2bf02cc2b2940b7551793c3737eab631d\n",
            "Successfully built lyricsgenius\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HOMiWHBMWoP",
        "colab_type": "code",
        "outputId": "f041bebb-b738-48cd-fc87-47dc85084148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "pip install lyricsgenius"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lyricsgenius in /usr/local/lib/python3.6/dist-packages (1.8.2)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from lyricsgenius) (2.21.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from lyricsgenius) (4.6.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49piQ_kaMXdy",
        "colab_type": "code",
        "outputId": "99c8a18d-d879-4211-e4aa-60c1e463123c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "pip install git+https://github.com/johnwmillr/LyricsGenius.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/johnwmillr/LyricsGenius.git\n",
            "  Cloning https://github.com/johnwmillr/LyricsGenius.git to /tmp/pip-req-build-_rego_3h\n",
            "  Running command git clone -q https://github.com/johnwmillr/LyricsGenius.git /tmp/pip-req-build-_rego_3h\n",
            "Requirement already satisfied (use --upgrade to upgrade): lyricsgenius==1.8.2 from git+https://github.com/johnwmillr/LyricsGenius.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from lyricsgenius==1.8.2) (4.6.0)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from lyricsgenius==1.8.2) (2.21.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius==1.8.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius==1.8.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius==1.8.2) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->lyricsgenius==1.8.2) (2.8)\n",
            "Building wheels for collected packages: lyricsgenius\n",
            "  Building wheel for lyricsgenius (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lyricsgenius: filename=lyricsgenius-1.8.2-py3-none-any.whl size=15038 sha256=a66d8a947ffc8b3589da07742424d0739d15347481f893ea9b9de7c7923a888b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2hcehjhl/wheels/58/60/c5/816302a2fe1ddc34a2bf02cc2b2940b7551793c3737eab631d\n",
            "Successfully built lyricsgenius\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKLBGdvhMbuM",
        "colab_type": "code",
        "outputId": "2b972e36-ed71-402a-dfc4-50967b78a411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import lyricsgenius\n",
        "genius = lyricsgenius.Genius(\"37_kPKwZgAEV2w8GjOYcuK-2c79H9YUx9PHkN_38aFDJa3Vdxzjveb7YDJ8TSCC1\")\n",
        "artist = genius.search_artist(\"Oxxxymiron\",  sort=\"title\")\n",
        "print(artist.songs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Searching for songs by Oxxxymiron...\n",
            "\n",
            "Song 1: \"16 Bars Acapella\"\n",
            "Song 2: \"Afterparty (Demo)\"\n",
            "Song 3: \"AI Ogon\"\n",
            "Song 4: \"Amplify and simplify (Freestyle)\"\n",
            "Song 5: \"CCTV\"\n",
            "Song 6: \"Darkside\"\n",
            "Song 7: \"Freestyle #1\"\n",
            "Song 8: \"Ganz Promo Tune\"\n",
            "Song 9: \"Hangover\"\n",
            "Song 10: \"HPL\"\n",
            "Song 11: \"IMPERIVM\"\n",
            "Song 12: \"Intro\"\n",
            "Song 13: \"OXXXYMIRON\"\n",
            "Song 14: \"Russky Cockney\"\n",
            "Song 15: \"Shade 45 Freestyle (Идея)\"\n",
            "Song 16: \"Street Freestyle battle\"\n",
            "Song 17: \"Ultima Thule\"\n",
            "Song 18: \"Unreleased Track\"\n",
            "Song 19: \"Unreleased Track 2\"\n",
            "Song 20: \"Unreleased Track 3\"\n",
            "Song 21: \"Unreleased Track 4\"\n",
            "Song 22: \"XXX SHOP\"\n",
            "Song 23: \"Башня из слоновой кости (Ivory Tower)\"\n",
            "Song 24: \"Биполярочка (Bipolarochka)\"\n",
            "Song 25: \"Больше Бена (Bigga Than Ben)\"\n",
            "Song 26: \"В бульбуляторе (In the Bong)\"\n",
            "Song 27: \"В говне (In Shit)\"\n",
            "Song 28: \"В долгий путь (1 раунд 17ib) (On a Long Journey)\"\n",
            "Song 29: \"Ветер перемен (2 раунд 17ib) (The Wind of Change)\"\n",
            "Song 30: \"Вечный жид (Everlasting Jew)\"\n",
            "Song 31: \"Витязи словоблудия (Уховертка) (The Knights of Verbiage (Earwig))\"\n",
            "Song 32: \"В книге всё было по-другому (4 раунд 17ib) (The Book Had It Different)\"\n",
            "Song 33: \"Волапюк (Volapük)\"\n",
            "Song 34: \"Восточный Мордор (East Mordor)\"\n",
            "Song 35: \"Всего лишь писатель (Just a Writer)\"\n",
            "Song 36: \"В стране женщин (In the country of women)\"\n",
            "Song 37: \"«Где нас нет» (”On the Other Side”)\"\n",
            "Song 38: \"Город под подошвой (City Under the Sole)\"\n",
            "Song 39: \"Город под подошвой (GPP tour version)\"\n",
            "Song 40: \"Девочка Пиздец (Devochka Pizdets) (2011/2012 demo)\"\n",
            "Song 41: \"Девочка Пиздец (Fucked Up Girl)\"\n",
            "Song 42: \"Дело нескольких минут (3 раунд 17ib) (A Matter of Minutes)\"\n",
            "Song 43: \"День физкультурника (Athlete’s Day)\"\n",
            "Song 44: \"Детектор лжи (Lie Detector)\"\n",
            "Song 45: \"До зимы (Before Winter)\"\n",
            "Song 46: \"До сих пор MC (Still MC)\"\n",
            "Song 47: \"Жук в муравейнике (Beetle in an anthill)\"\n",
            "Song 48: \"Интро (Intro II)\"\n",
            "Song 49: \"Йети и дети (Yeti and children)\"\n",
            "Song 50: \"Каменный Век Русской Поэзии (Stone Age of Russian Poetry)\"\n",
            "Song 51: \"Кем ты стал? (What Had You Become?)\"\n",
            "Song 52: \"Колыбельная (Lullaby)\"\n",
            "Song 53: \"Крокодиловы слёзы (Crocodile tears)\"\n",
            "Song 54: \"Литература (Фристайл) (Literature (Freestyle))\"\n",
            "Song 55: \"Лондонград (OST Londongrad)\"\n",
            "Song 56: \"Лондон против всех (London Vs. Everyone)\"\n",
            "Song 57: \"Лондон против всех, ч. 2 (London Vs. Everyone, p. 2)\"\n",
            "Song 58: \"Мой менталитет (Oi Oi)\"\n",
            "Song 59: \"Накануне (On the Eve)\"\n",
            "Song 60: \"На куски (Demo) (In Pieces)\"\n",
            "Song 61: \"Неваляшка (Tumbler Toy)\"\n",
            "Song 62: \"Не говори ни слова (Ne govori ni slova)\"\n",
            "Song 63: \"Не от мира сего (Not of This World)\"\n",
            "Song 64: \"Не с начала (Not From the Beginning)\"\n",
            "Song 65: \"Нет связи (No connection)\"\n",
            "Song 66: \"Пародия на группу Centr freestyle\"\n",
            "Song 67: \"Переплетено (Interlaced)\"\n",
            "Song 68: \"Песенка Гремлина (Gremlin’s Song)\"\n",
            "Song 69: \"Песенка про Шахматиста (Song About a Chess Player)\"\n",
            "Song 70: \"«Полигон» (“Butts”)\"\n",
            "Song 71: \"Последний звонок (Last Call)\"\n",
            "Song 72: \"Привет со дна  (Hello from the Bottom)\"\n",
            "Song 73: \"Признаки Жизни (Signs of Life)\"\n",
            "Song 74: \"Пролив Дрейка (Drake Passage)\"\n",
            "Song 75: \"Раунд 1: 3030 год (Round 1: 3030 year)\"\n",
            "Song 76: \"Слово мэра (Mayor’s Word)\"\n",
            "Song 77: \"Спонтанное самовозгорание (Spontaneous Combustion)\"\n",
            "Song 78: \"Судьба моралиста (The fate of the moralist)\"\n",
            "Song 79: \"Тайные желания (Secret Desires)\"\n",
            "Song 80: \"Тентакли (Tentacles)\"\n",
            "Song 81: \"Три монолога (Three Monologues)\"\n",
            "Song 82: \"Хитиновый покров (Chitin Shell)\"\n",
            "Song 83: \"Ходят слухи (Hodyat sluhi)\"\n",
            "Song 84: \"Цифры и цвета (Numbers & colors)\"\n",
            "Song 85: \"Чёртово колесо (Chyortovo Koleso)\"\n",
            "Song 86: \"Шалом (Shalom)\"\n",
            "Song 87: \"Я Хейтер (I’m a Hater)\"\n",
            "Song 88: \"Ящик фокусника (Yaschik Fokusnika)\"\n",
            "Done. Found 88 songs.\n",
            "[('16 Bars Acapella', 'Oxxxymiron'), ('Afterparty (Demo)', 'Oxxxymiron'), ('AI Ogon', 'Oxxxymiron'), ('Amplify and simplify (Freestyle)', 'Oxxxymiron'), ('CCTV', 'Oxxxymiron'), ('Darkside', 'Oxxxymiron'), ('Freestyle #1', 'Oxxxymiron'), ('Ganz Promo Tune', 'Oxxxymiron'), ('Hangover', 'Oxxxymiron'), ('HPL', 'Oxxxymiron'), ('IMPERIVM', 'Oxxxymiron'), ('Intro', 'Oxxxymiron'), ('OXXXYMIRON', 'Oxxxymiron'), ('Russky Cockney', 'Oxxxymiron'), ('Shade 45 Freestyle (Идея)', 'Oxxxymiron'), ('Street Freestyle battle', 'Oxxxymiron'), ('Ultima Thule', 'Oxxxymiron'), ('Unreleased Track', 'Oxxxymiron'), ('Unreleased Track 2', 'Oxxxymiron'), ('Unreleased Track 3', 'Oxxxymiron'), ('Unreleased Track 4', 'Oxxxymiron'), ('XXX SHOP', 'Oxxxymiron'), ('Башня из слоновой кости (Ivory Tower)', 'Oxxxymiron'), ('Биполярочка (Bipolarochka)', 'Oxxxymiron'), ('Больше Бена (Bigga Than Ben)', 'Oxxxymiron'), ('В бульбуляторе (In the Bong)', 'Oxxxymiron'), ('В говне (In Shit)', 'Oxxxymiron'), ('В долгий путь (1 раунд 17ib) (On a Long Journey)', 'Oxxxymiron'), ('Ветер перемен (2 раунд 17ib) (The Wind of Change)', 'Oxxxymiron'), ('Вечный жид (Everlasting Jew)', 'Oxxxymiron'), ('Витязи словоблудия (Уховертка) (The Knights of Verbiage (Earwig))', 'Oxxxymiron'), ('В книге всё было по-другому (4 раунд 17ib) (The Book Had It Different)', 'Oxxxymiron'), ('Волапюк (Volapük)', 'Oxxxymiron'), ('Восточный Мордор (East Mordor)', 'Oxxxymiron'), ('Всего лишь писатель (Just a Writer)', 'Oxxxymiron'), ('В стране женщин (In the country of women)', 'Oxxxymiron'), ('«Где нас нет» (”On the Other Side”)', 'Oxxxymiron'), ('Город под подошвой (City Under the Sole)', 'Oxxxymiron'), ('Город под подошвой (GPP tour version)', 'Oxxxymiron'), ('Девочка Пиздец (Devochka Pizdets) (2011/2012 demo)', 'Oxxxymiron'), ('Девочка Пиздец (Fucked Up Girl)', 'Oxxxymiron'), ('Дело нескольких минут (3 раунд 17ib) (A Matter of Minutes)', 'Oxxxymiron'), ('День физкультурника (Athlete’s Day)', 'Oxxxymiron'), ('Детектор лжи (Lie Detector)', 'Oxxxymiron'), ('До зимы (Before Winter)', 'Oxxxymiron'), ('До сих пор MC (Still MC)', 'Oxxxymiron'), ('Жук в муравейнике (Beetle in an anthill)', 'Oxxxymiron'), ('Интро (Intro II)', 'Oxxxymiron'), ('Йети и дети (Yeti and children)', 'Oxxxymiron'), ('Каменный Век Русской Поэзии (Stone Age of Russian Poetry)', 'Oxxxymiron'), ('Кем ты стал? (What Had You Become?)', 'Oxxxymiron'), ('Колыбельная (Lullaby)', 'Oxxxymiron'), ('Крокодиловы слёзы (Crocodile tears)', 'Oxxxymiron'), ('Литература (Фристайл) (Literature (Freestyle))', 'Oxxxymiron'), ('Лондонград (OST Londongrad)', 'Oxxxymiron'), ('Лондон против всех (London Vs. Everyone)', 'Oxxxymiron'), ('Лондон против всех, ч. 2 (London Vs. Everyone, p. 2)', 'Oxxxymiron'), ('Мой менталитет (Oi Oi)', 'Oxxxymiron'), ('Накануне (On the Eve)', 'Oxxxymiron'), ('На куски (Demo) (In Pieces)', 'Oxxxymiron'), ('Неваляшка (Tumbler Toy)', 'Oxxxymiron'), ('Не говори ни слова (Ne govori ni slova)', 'Oxxxymiron'), ('Не от мира сего (Not of This World)', 'Oxxxymiron'), ('Не с начала (Not From the Beginning)', 'Oxxxymiron'), ('Нет связи (No connection)', 'Oxxxymiron'), ('Пародия на группу Centr freestyle', 'Oxxxymiron'), ('Переплетено (Interlaced)', 'Oxxxymiron'), ('Песенка Гремлина (Gremlin’s Song)', 'Oxxxymiron'), ('Песенка про Шахматиста (Song About a Chess Player)', 'Oxxxymiron'), ('«Полигон» (“Butts”)', 'Oxxxymiron'), ('Последний звонок (Last Call)', 'Oxxxymiron'), ('Привет со дна  (Hello from the Bottom)', 'Oxxxymiron'), ('Признаки Жизни (Signs of Life)', 'Oxxxymiron'), ('Пролив Дрейка (Drake Passage)', 'Oxxxymiron'), ('Раунд\\xa01: 3030 год (Round 1: 3030 year)', 'Oxxxymiron'), ('Слово мэра (Mayor’s Word)', 'Oxxxymiron'), ('Спонтанное самовозгорание (Spontaneous Combustion)', 'Oxxxymiron'), ('Судьба моралиста (The fate of the moralist)', 'Oxxxymiron'), ('Тайные желания (Secret Desires)', 'Oxxxymiron'), ('Тентакли (Tentacles)', 'Oxxxymiron'), ('Три монолога (Three Monologues)', 'Oxxxymiron'), ('Хитиновый покров (Chitin Shell)', 'Oxxxymiron'), ('Ходят слухи (Hodyat sluhi)', 'Oxxxymiron'), ('Цифры и цвета (Numbers & colors)', 'Oxxxymiron'), ('Чёртово колесо (Chyortovo Koleso)', 'Oxxxymiron'), ('Шалом (Shalom)', 'Oxxxymiron'), ('Я Хейтер (I’m a Hater)', 'Oxxxymiron'), ('Ящик фокусника (Yaschik Fokusnika)', 'Oxxxymiron')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI1rv1erMgcB",
        "colab_type": "code",
        "outputId": "82c0bfc5-1a67-4350-f6b8-9642838bd58c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "artist.save_lyrics(extension='txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrote `Lyrics_Oxxxymiron.txt`\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRRovyVOl--o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lyricsoxxxy=open(\"Lyrics_Oxxxymiron.txt\",\"r\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W0eQBQTrJ_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_huoIvowJ8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def gen(model,seq,max_len = 20):\n",
        "    ''' Generates a sequence given a string seq using specified model until the total sequence length\n",
        "    reaches max_len'''\n",
        "    # Tokenize the input string\n",
        "    tokenized_sent = tokenizer.texts_to_sequences([seq])\n",
        "    max_len = max_len+len(tokenized_sent[0])\n",
        "    # If sentence is not as long as the desired sentence length, we need to 'pad sequence' so that\n",
        "    # the array input shape is correct going into our LSTM. the `pad_sequences` function adds \n",
        "    # zeroes to the left side of our sequence until it becomes 19 long, the number of input features.\n",
        "    while len(tokenized_sent[0]) < max_len:\n",
        "        padded_sentence = pad_sequences(tokenized_sent[-19:],maxlen=19)\n",
        "        op = model.predict(np.asarray(padded_sentence).reshape(1,-1))\n",
        "        tokenized_sent[0].append(op.argmax()+1)\n",
        "        \n",
        "    return \" \".join(map(lambda x : reverse_word_map[x],tokenized_sent[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERj257TPmnkd",
        "colab_type": "code",
        "outputId": "fa30afb0-fc89-4be4-ecde-c5a394c182ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(lyricsoxxxy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_io.TextIOWrapper name='Lyrics_Oxxxymiron.txt' mode='r' encoding='UTF-8'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERAJDrPwnRvs",
        "colab_type": "code",
        "outputId": "a7328b92-7b58-4476-bb41-6283858d0727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install tokenizer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizer\n",
            "  Downloading tokenizer-2.0.5-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 2.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizer\n",
            "Successfully installed tokenizer-2.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIi9mhtunvOS",
        "colab_type": "code",
        "outputId": "5c15de94-085c-4fdc-ef43-0a78ad771698",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy\n",
        "import sys\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpCLT5r7nPsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lyricsoxxxy = open(\"Lyrics_Oxxxymiron.txt\").read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBKR5g2DND2P",
        "colab_type": "code",
        "outputId": "c14fa216-0c7d-4e0a-a25d-b0ff0c8a9e55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download(\"stopwords\") "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DQxCH0-MUzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_words(input):\n",
        "    # lowercase everything to standardize it\n",
        "    input = input.lower()\n",
        "\n",
        "    # instantiate the tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(input)\n",
        "\n",
        "    # if the created token isn't in the stop words, make it part of \"filtered\"\n",
        "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
        "    return \" \".join(filtered)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAgmdu2rMiNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_inputs = tokenize_words(lyricsoxxxy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXF6pQ3SMorO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = sorted(list(set(processed_inputs))) #cause we get numbers which represent the characters.\n",
        "char_to_num = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-8PzB_aNZ6T",
        "colab_type": "code",
        "outputId": "a2600d53-915f-40ea-a7e5-94cdb28ddd3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "input_len = len(processed_inputs)\n",
        "vocab_len = len(chars)\n",
        "print (\"Total number of characters:\", input_len)\n",
        "print (\"Total vocab:\", vocab_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of characters: 201196\n",
            "Total vocab: 76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqfUcz7BNnsv",
        "colab_type": "text"
      },
      "source": [
        "Now we see can see the lenth of our variables. Firstly, w'll try to make textes based only on oxxxy texts. Than mb expand in case we need it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeVaxef6Nliu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 100 #we'll change the legnth\n",
        "x_data = [] #to store our inputs\n",
        "y_data = [] #to store our outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILpEdTwCOR5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loop through inputs, start at the beginning and go until we hit\n",
        "# the final character we can create a sequence out of\n",
        "for i in range(0, input_len - seq_length, 1):\n",
        "    # Define input and output sequences\n",
        "    # Input is the current character plus desired sequence length\n",
        "    in_seq = processed_inputs[i:i + seq_length]\n",
        "\n",
        "    # Out sequence is the initial character plus total sequence length\n",
        "    out_seq = processed_inputs[i + seq_length]\n",
        "\n",
        "    # We now convert list of characters to integers based on\n",
        "    # previously and add the values to our lists\n",
        "    x_data.append([char_to_num[char] for char in in_seq])\n",
        "    y_data.append(char_to_num[out_seq])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUZY9uM8OYTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We now have our training data features and labels, stored as x_data and y_data."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llV8vs7wOgbK",
        "colab_type": "code",
        "outputId": "338760e7-b49f-4f3e-e200-4e16bf41cb34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_patterns = len(x_data)\n",
        "print (\"Total Patterns:\", n_patterns) #to check"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns: 201096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2db6Ne7Olj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this is the same number we had "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH9DH9xfOxEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
        "X = X/float(vocab_len)\n",
        "#processed numpy array that our network can use. We'll also need to convert the numpy array values into floats "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb9E2V0lOxhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np_utils.to_categorical(y_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rez-EfTfQHK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.python.keras.layers import Dropout\n",
        "from tensorflow.python.keras.layers import LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yHI3VnVO7HZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NM5y3NlPQ4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOEatqMhQdjg",
        "colab_type": "code",
        "outputId": "4ef60f68-d75d-4db0-cde4-4255c09b2f08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.fit(X, y, epochs=22, batch_size=256)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/22\n",
            "786/786 [==============================] - 2831s 4s/step - loss: 3.1986\n",
            "Epoch 2/22\n",
            "786/786 [==============================] - 2778s 4s/step - loss: 2.8955\n",
            "Epoch 3/22\n",
            "786/786 [==============================] - 2778s 4s/step - loss: 2.8191\n",
            "Epoch 4/22\n",
            "786/786 [==============================] - 2846s 4s/step - loss: 2.7704\n",
            "Epoch 5/22\n",
            "786/786 [==============================] - 2921s 4s/step - loss: 2.7316\n",
            "Epoch 6/22\n",
            "786/786 [==============================] - 2839s 4s/step - loss: 2.6921\n",
            "Epoch 7/22\n",
            "475/786 [=================>............] - ETA: 18:32 - loss: 2.6613"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rptOR8BjQxGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "#convert the output of the model back into numbers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0BVupEORRTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = numpy.random.randint(0, len(x_data) - 1)\n",
        "pattern = x_data[start]\n",
        "print(\"Random Seed:\")\n",
        "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcQv8V_eRny3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(1000):\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(vocab_len)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = num_to_char[index]\n",
        "    seq_in = [num_to_char[value] for value in pattern]\n",
        "\n",
        "    sys.stdout.write(result)\n",
        "\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "    #We'll ask the model to predict what comes next based off of the random seed, \n",
        "    #convert the output numbers to characters and then append it to the pattern, \n",
        "    #which is our list of generated characters plus the initial seed:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhwmCMplSC8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKJzZycHM4Lw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ">>> from nltk import lm\n",
        ">>> help(lm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6-ZiSv3SpBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk import sent_tokenize\n",
        "from nltk import tokenize\n",
        "from nltk import lm\n",
        "from nltk.tokenize import LineTokenizer\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.util import pad_sequence\n",
        "from nltk.util import bigrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import everygrams\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.lm.preprocessing import flatten\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm import Laplace\n",
        "from nltk.lm import KneserNeyInterpolated\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCFdzAFqRw5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = train_test_split(processed_inputs, test_size=0.2, random_state=135)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuDLU69TR8MC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data00,_ = padded_everygram_pipeline(2, test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUOd7JgjSN04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_gen00 = []\n",
        "\n",
        "for line in test_data00:\n",
        "  test_get00.extend(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgywiRnFSWlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Perplexity is: {}'.format(model.perplexity(test_gen00)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykk5zrAYOF1n",
        "colab_type": "text"
      },
      "source": [
        "Initially I tried a smaller number of epochs (3, 4, 6) as it took a lot of time.  It generated some rap but it made not that much sense. Now I try to make more than 20 epochs annd it should be enough for the model to train well.\n",
        "Probably, I will upload this part even before the final epoch is done because it's been generating for more than 10 hours so far and the deadline is coming.\n",
        "\n",
        "This is the part devoted to construction of a NN. The second onne will be with language models. (here the concept of lm should be implemented only to estimate perplexity)."
      ]
    }
  ]
}